{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-zeidi/Info_Ret/blob/main/HVD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W17WDy1OhKZB"
      },
      "source": [
        "# **Data Generation and Preprocessing**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rq9sdx1hhF8"
      },
      "source": [
        "We will take the following steps to preprocess the data:\n",
        "\n",
        "\n",
        "\n",
        "1.   Match data with the corresponding labels in one dataframe\n",
        "2.   transform input data and create column text: Premist + Conclusion + Stance\n",
        "\n",
        "\n",
        "\n",
        "**Input:** *arguments-training.tsv, arguments-validation.ts*\n",
        "\n",
        "**Output:** *train_full.csv, validation_full.csv*\n",
        "\n",
        "We will use this structure for 4 different datasets that we want to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mgmqn7XiK3Y"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path ='/content/drive/MyDrive/InfoRet/data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3A0vnjMj8wE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "def create_data_file(path_to_arguments, path_to_labels, output_path=None, drop_duplicates=True):\n",
        "    \"\"\"\n",
        "    Create a data file by merging arguments and labels from the corresponding_files from two files using pandas library.\n",
        "\n",
        "    Args:\n",
        "        path_to_arguments (str): File path to the template file.\n",
        "        path_to_labels (str): File path to the labels file.\n",
        "        output_path (str, optional): File path to save the output data file.\n",
        "        drop_duplicates (bool, optional): Whether to drop duplicate rows in the output data file.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Merged data with duplicate rows dropped (if drop_duplicates is True).\n",
        "    \"\"\"\n",
        "\n",
        "    # Load Arguments & Concat Premise Stance Conclusion\n",
        "    df_arguments = pd.read_csv(path_to_arguments, sep='\\t', quoting=csv.QUOTE_NONE, encoding=\"utf-8\", header=0)\n",
        "    df_arguments[\"text\"] = df_arguments[\"Premise\"] + \" \" + df_arguments[\"Stance\"] + \" \" + df_arguments[\"Conclusion\"]\n",
        "\n",
        "    # Load Labels & Convert to Ints and Create Column with List of Label Names\n",
        "    df_labels = pd.read_csv(path_to_labels, sep='\\t', quoting=csv.QUOTE_NONE, encoding=\"utf-8\", header=0)\n",
        "\n",
        "    for col in df_labels.columns[1:]:\n",
        "        df_labels[col] = df_labels[col].astype(int)\n",
        "\n",
        "    df_labels['category'] = df_labels.apply(lambda row: [col for col in df_labels.columns if row[col] == 1], axis=1)\n",
        "\n",
        "    # Reorder Columnames\n",
        "    cols = df_labels.columns.tolist()\n",
        "    new_cols = cols[:-1]      # All columns except the last one\n",
        "    new_cols.insert(1, cols[-1])  # Insert the last column to the second position\n",
        "    df_labels = df_labels[new_cols]\n",
        "\n",
        "    # Merge Data based on ID\n",
        "    df_merged = pd.merge(df_arguments, df_labels, on=[\"Argument ID\"])\n",
        "\n",
        "    if drop_duplicates:\n",
        "        df_merged = df_merged.drop_duplicates(subset=[\"text\"])\n",
        "\n",
        "    if output_path:\n",
        "        df_merged.to_csv(output_path)\n",
        "\n",
        "    return df_merged\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y39eSogI--0_"
      },
      "source": [
        "## **First data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7PajbrFkqxd"
      },
      "outputs": [],
      "source": [
        "train_df = create_data_file(data_path + \"arguments-training.tsv\", data_path + \"labels-training.tsv\", data_path + \"generated_data/\"+\"arg_training_label_task.csv\")\n",
        "val_df = create_data_file(data_path + \"arguments-validation.tsv\", data_path + \"labels-validation.tsv\", data_path +\"generated_data/\"+ \"arg_validation_label_task.csv\")\n",
        "test_df = create_data_file(data_path + \"arguments-test.tsv\", data_path + \"labels-test.tsv\", data_path +\"generated_data/\"+ \"arg_test_label_task.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKn8ONsO_JWg"
      },
      "source": [
        "##**Second data** (Zhihu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3WiABIo-63J"
      },
      "outputs": [],
      "source": [
        "test_df = create_data_file(data_path + \"arguments-validation-zhihu.tsv\", data_path + \"labels-validation-zhihu.tsv\", data_path +\"generated_data/\"+ \"arg_validation_label_zhihu_task.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2P_CdZDDGh1"
      },
      "source": [
        "##**Third data**  (nahjalbalagha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qIcOvroDkdp"
      },
      "outputs": [],
      "source": [
        "test_df = create_data_file(data_path + \"arguments-test-nahjalbalagha.tsv\", data_path + \"labels-test-nahjalbalagha.tsv\", data_path +\"generated_data/\"+ \"arg_test_label_nahjalbalagha_task.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T29CxYbTzR_U"
      },
      "source": [
        "##**Fourth data** (nyt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmOvSlumzhYg"
      },
      "outputs": [],
      "source": [
        "test_df = create_data_file(data_path + \"arguments-test-nyt.tsv\", data_path + \"labels-test-nyt.tsv\", data_path +\"generated_data/\"+ \"arg_test_label_nyt_task.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf-4DYAMk-aF"
      },
      "outputs": [],
      "source": [
        "print(\"len of train_df:\",len(train_df))\n",
        "print(\"len of val_df:\",len(val_df))\n",
        "print(\"len of test_df:\",len(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPaEX8JBnhfH"
      },
      "source": [
        "# **Train Language Models to Detect Human Values in Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7sGXojfni5O"
      },
      "outputs": [],
      "source": [
        "!pip install simpletransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsfsSNtZZT8K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import logging\n",
        "import json\n",
        "from argparse import ArgumentParser\n",
        "from simpletransformers.classification import (\n",
        "    MultiLabelClassificationModel, MultiLabelClassificationArgs\n",
        ")\n",
        "from sklearn.metrics import f1_score, precision_recall_curve, precision_recall_fscore_support, classification_report\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEkJMB3qZeIY"
      },
      "source": [
        "\n",
        "##**Data preparation for the SimpleTransformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh-5MTLeZvO0"
      },
      "source": [
        " Data preparation for the SimpleTransformers\n",
        "If the dataframe has a header row, the text column should have the heading text and the labels column should have the heading labels.\n",
        "\n",
        "https://simpletransformers.ai/docs/classification-data-formats/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqKHIWijZ1cU"
      },
      "outputs": [],
      "source": [
        "def prepare_labels(df):\n",
        "    labels = []\n",
        "    for _, row in df.iterrows():\n",
        "        text = row['text']\n",
        "        label = row[df.columns[7:]].tolist()\n",
        "\n",
        "        assert (len(label) == 20)\n",
        "        labels.append([text, label])\n",
        "\n",
        "    processed_df = pd.DataFrame(labels, columns=['text', 'labels'])\n",
        "    return processed_df\n",
        "\n",
        "\n",
        "train = pd.read_csv(data_path + \"generated_data/\"+\"arg_training_label_task.csv\")\n",
        "valid = pd.read_csv(data_path + \"generated_data/\"+\"arg_validation_label_task.csv\")\n",
        "test = pd.read_csv(data_path + \"generated_data/\"+\"arg_test_label_task.csv\")\n",
        "#test = pd.read_csv(data_path + \"generated_data/\"+\"arg_validation_label_zhihu_task.csv\")\n",
        "#test = pd.read_csv(data_path + \"generated_data/\"+\"arg_test_label_nahjalbalagha_task.csv\")\n",
        "#test = pd.read_csv(data_path + \"generated_data/\"+\"arg_test_label_nyt_task.csv\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"prepare labels ...\")\n",
        "train_df = prepare_labels(train)\n",
        "val_df = prepare_labels(valid)\n",
        "test_df = prepare_labels(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f1VEw5qaIJ6"
      },
      "outputs": [],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB1nrUYuaREc"
      },
      "source": [
        "## **Model configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuhqQR5BaUbZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "cuda_available = torch.cuda.is_available()\n",
        "\n",
        "# Print the result\n",
        "if cuda_available:\n",
        "    print(\"CUDA is available. Using GPU.\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Using CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_G_l4OEhirp"
      },
      "outputs": [],
      "source": [
        "# Optional model configuration\n",
        "models_path ='/content/drive/MyDrive/InfoRet/models/'\n",
        "#model_type=\"bert\" #mlm+nsp  #alternative model_type : roberta/ albert/ ... (ref to https://simpletransformers.ai/docs/classification-specifics/#supported-model-types)\n",
        "#model_name=\"google-bert/bert-base-uncased\" #This may be a Hugging Face Transformers compatible pre-trained model, a community model, or the path to a directory containing model files. (ref to https://simpletransformers.ai/docs/classification-models/ )\n",
        "\n",
        "model_type=\"roberta\" #mlm _restrucer of model\n",
        "model_name=\"FacebookAI/roberta-base\"\n",
        "\n",
        "#model_type=\"albert\" #mlm+sop _restrucer of model\n",
        "#model_name=\"albert/albert-base-v2\"\n",
        "\n",
        "model_args = MultiLabelClassificationArgs(num_train_epochs=12)\n",
        "model_args.train_batch_size =16\n",
        "model_args.evaluate_during_training = True\n",
        "model_args.overwrite_output_dir = True\n",
        "model_args.output_dir = f\"{models_path}{model_name}\"\n",
        "model_args.best_model_dir = f\"{models_path}{model_name}/best_model\"\n",
        "model_args.save_model_every_epoch = True\n",
        "model_args.save_eval_checkpoints = False\n",
        "# model_args.report_to=\"wandb\" # enable logging to W&B\n",
        "# model_args.run_name=str(model_name),  # name of the W&B run (optional)\n",
        "\n",
        "# Create a MultiLabelClassificationModel\n",
        "model = MultiLabelClassificationModel(\n",
        "    model_type,\n",
        "    model_name,\n",
        "    num_labels=20,\n",
        "    args=model_args,\n",
        "    use_cuda=True\n",
        "\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oQpFejJm7Nc"
      },
      "source": [
        "Some weights of BertForMultiLabelSequenceClassification were not initialized\n",
        "from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3JXe8FSm-YD"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model.train_model(train_df, eval_df=val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RU-llrmpslk"
      },
      "outputs": [],
      "source": [
        "# test result for each lable\n",
        "values = [\n",
        "    \"Self-direction: thought\",\n",
        "    \"Self-direction: action\",\n",
        "    \"Stimulation\",\n",
        "    \"Hedonism\",\n",
        "    \"Achievement\",\n",
        "    \"Power: dominance\",\n",
        "    \"Power: resources\",\n",
        "    \"Face\",\n",
        "    \"Security: personal\",\n",
        "    \"Security: societal\",\n",
        "    \"Tradition\",\n",
        "    \"Conformity: rules\",\n",
        "    \"Conformity: interpersonal\",\n",
        "    \"Humility\",\n",
        "    \"Benevolence: caring\",\n",
        "    \"Benevolence: dependability\",\n",
        "    \"Universalism: concern\",\n",
        "    \"Universalism: nature\",\n",
        "    \"Universalism: tolerance\",\n",
        "    \"Universalism: objectivity\"\n",
        "]\n",
        "\n",
        "\n",
        "def write_pr_rc_f1_to_file(path, pr, rc, f1):\n",
        "    with open(f'{path}/metrics.txt', \"w\") as f:\n",
        "        f.write(f\"pr: {str(pr)}\")\n",
        "        f.write(\"\\n\")\n",
        "        f.write(f\"rc:  {str(rc)}\")\n",
        "        f.write(\"\\n\")\n",
        "        f.write(f\"f1: {str(f1)}\")\n",
        "\n",
        "\n",
        "\n",
        "def write_text(filename, text):\n",
        "    with open(filename, \"w\") as f:\n",
        "        # f.write(text)\n",
        "        f.write(json.dumps(text))\n",
        "        f.write('\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "result_path =f'/content/drive/MyDrive/InfoRet/models/{model_name}/best_model/'\n",
        "\n",
        "predictions, _ = model.predict(test_df['text'].tolist())\n",
        "gold_label = test_df['labels'].tolist()\n",
        "pr, rc, f1, sup = precision_recall_fscore_support(gold_label, predictions, average=\"macro\",  zero_division=1)\n",
        "write_pr_rc_f1_to_file(path=f\"{result_path}\", pr=pr, rc=rc, f1=f1)\n",
        "\n",
        "report = classification_report(gold_label, predictions, target_names=values, output_dict=True,  zero_division=1)\n",
        "print(report)\n",
        "write_text(f\"{result_path}/classification_report_metric.txt\", report)\n",
        "\n",
        "print(f\"clssification_on_{model_name} is done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU8duTljE5Na"
      },
      "source": [
        "## **Make confusion matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpGm7gOiorMZ"
      },
      "outputs": [],
      "source": [
        "# for each layer\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "# Compute the multilabel confusion matrix\n",
        "mcm = multilabel_confusion_matrix(gold_label, predictions)\n",
        "confusion_matrices = {}\n",
        "# Print the confusion matrix for each class\n",
        "for i, label in enumerate(values):\n",
        "    # print(f\"Confusion Matrix for {label}:\")\n",
        "    # print(mcm[i])\n",
        "    confusion_matrices[label] = mcm[i]\n",
        "\n",
        "\n",
        "confusion_matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAf4ftu3r_os"
      },
      "outputs": [],
      "source": [
        "# total confusion matrix\n",
        "import numpy as np\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "# Compute the multilabel confusion matrix for each class\n",
        "mcm = multilabel_confusion_matrix(gold_label, predictions)\n",
        "\n",
        "# Aggregate the confusion matrices across all classes\n",
        "total_mcm = np.sum(mcm, axis=0)\n",
        "\n",
        "# Print the total multilabel confusion matrix\n",
        "print(\"Total Multilabel Confusion Matrix:\")\n",
        "print(total_mcm)\n",
        "\n",
        "total_mcm=pd.DataFrame(total_mcm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9rI1l3k9Qjz"
      },
      "outputs": [],
      "source": [
        "#saving confusion matrix\n",
        "\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_firstdataBBUlabels.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_zhihutdataBBUlabels.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_nhjtdataBBUlabels.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_nyttdataBBUlabels.csv'\n",
        "\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_firstdataRoBERTalabels.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_zhihudataRoBERTalabels.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_nhjdataRoBERTalabels.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_nytdataRoBERTalabels.csv'\n",
        "\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_firstdataAlBERTlabels.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_zhihudataAlBERTlabels.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_nhjdataAlBERTlabels.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_nytdataAlBERTlabels.csv'\n",
        "\n",
        "confusion_matrix.to_csv(csv_file_path, index=False)\n",
        "\n",
        "#BERT\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_firstdataBBUtotal.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_zhihudataBBUtotal.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_nhjdataBBUtotal.csv'\n",
        "csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_nytdataBBUtotal.csv'\n",
        "\n",
        "#RoBERTa\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_firstdataRoBERTatotal.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_zhihudataRoBERTatotal.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_nhjdataRoBERTatotal.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_nytdataRoBERTatotal.csv'\n",
        "\n",
        "#AlBERT\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_firstdataAlBERTtotal.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_zhihudataAlBERTtotal.csv'\n",
        "# csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_nhjdataAlBERTtotal.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/matrix/confusion_matrices_nytdataAlBERTtotal.csv'\n",
        "\n",
        "\n",
        "\n",
        "total_mcm.to_csv(csv_file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf-AZZvJFniC"
      },
      "source": [
        "## **Making report table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQlGcq5TwJKm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def report_to_dataframe(report):\n",
        "    data = []\n",
        "    for label, metrics in report.items():\n",
        "        data.append([label, metrics['precision'], metrics['recall'], metrics['f1-score'], metrics['support']])\n",
        "    df = pd.DataFrame(data, columns=['Label', 'Precision', 'Recall', 'F1-Score', 'Support'])\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "#merged_df = pd.DataFrame({'all score': [pr, rc, f1, sup]})\n",
        "\n",
        "# Convert report to DataFrame\n",
        "df = report_to_dataframe(report)\n",
        "\n",
        "# New row to be added at the beginning of the DataFrame\n",
        "new_row = {\"Label\": \"all\", \"Precision\": pr, \"Recall\": rc, \"F1-Score\": f1, \"Support\": sup}\n",
        "\n",
        "# Convert the dictionary into a DataFrame\n",
        "All = pd.DataFrame([new_row])\n",
        "df2 = pd.DataFrame(All)\n",
        "merged_df = pd.concat([df2, df], ignore_index=True)\n",
        "a=merged_df.T\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah376WTUEWkG"
      },
      "outputs": [],
      "source": [
        "#saving report(total)\n",
        "\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/all/allfirstdataBBU.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/all/allzhihudataBBU.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/all/allnhjdataBBU.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/all/allnytdataBBU.csv'\n",
        "\n",
        "# csv_file_path = '/content/drive/My Drive/InfoRet/results/all/allfirstdataRoBERTa.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/all/allzhihudataRoBERTa.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/all/allnhjdataRoBERTa.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/all/allnytdataRoBERTa.csv'\n",
        "\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/all/allfirstdataAlBERT.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/all/allzhihudataAlBERT.csv'\n",
        "# csv_file_path = '/content/drive/My Drive/InfoRet/results/all/allnhjdataAlBERT.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/all/allnytdataAlBERT.csv'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "a.to_csv(csv_file_path, index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utNiU06buj6m"
      },
      "outputs": [],
      "source": [
        "values = [\"All\",\n",
        "    \"Self-direction: thought\",\n",
        "    \"Self-direction: action\",\n",
        "    \"Stimulation\",\n",
        "    \"Hedonism\",\n",
        "    \"Achievement\",\n",
        "    \"Power: dominance\",\n",
        "    \"Power: resources\",\n",
        "    \"Face\",\n",
        "    \"Security: personal\",\n",
        "    \"Security: societal\",\n",
        "    \"Tradition\",\n",
        "    \"Conformity: rules\",\n",
        "    \"Conformity: interpersonal\",\n",
        "    \"Humility\",\n",
        "    \"Benevolence: caring\",\n",
        "    \"Benevolence: dependability\",\n",
        "    \"Universalism: concern\",\n",
        "    \"Universalism: nature\",\n",
        "    \"Universalism: tolerance\",\n",
        "    \"Universalism: objectivity\",\"\",\"\",\"\",\"\"\n",
        "]\n",
        "row_to_keep = a.iloc[3]\n",
        "# Convert the selected row to a DataFrame\n",
        "answer = pd.DataFrame([row_to_keep]).rename(index={3: 'F1-Score'})\n",
        "# Display DataFrame with only one row\n",
        "answer.columns=values\n",
        "answer\n",
        "#BERT #done\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/firstdataBBU.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/zhihudataBBU.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/nhjdataBBU.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/nytdataBBU.csv'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#AlBERT # done\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/firstdataAlBERT.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/zhihudataAlBERT.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/nhjdataAlBERT.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/nytdataAlBERT.csv'\n",
        "\n",
        "#RoBERTA #done\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/firstdataRoBERTA.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/zhihudataRoBERTA.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/nhjdataRoBERTA.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/nytdataRoBERTA.csv'\n",
        "\n",
        "# Save DataFrame to CSV file in Google Drive\n",
        "answer.to_csv(csv_file_path, index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaWJ2nmq4lF0"
      },
      "outputs": [],
      "source": [
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNa7cRONKw1m"
      },
      "source": [
        "# **Comparing Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEhJF2jzJvp7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#'/content/drive/MyDrive/InfoRet/data/'\n",
        "# Replace 'file_path.csv' with the path to your CSV file\n",
        "file_path = '/content/drive/MyDrive/InfoRet/results/'\n",
        "\n",
        "#bertbase\n",
        "allBERT = pd.read_csv(file_path+'allBERT.csv')\n",
        "\n",
        "firstdataBBU = pd.read_csv(file_path+'firstdataBBU.csv')\n",
        "zhihudataBBU=pd.read_csv(file_path+'zhihudataBBU.csv')\n",
        "nhjdataBBU=pd.read_csv(file_path+'nhjdataBBU.csv')\n",
        "nytdataBBU=pd.read_csv(file_path+'nytdataBBU.csv')\n",
        "\n",
        "\n",
        "#AlBERT\n",
        "allAlBERT = pd.read_csv(file_path+'allAlBERT.csv')\n",
        "\n",
        "firstdataAlBERT = pd.read_csv(file_path+'firstdataAlBERT.csv')\n",
        "zhihudataAlBERT = pd.read_csv(file_path+'zhihudataAlBERT.csv')\n",
        "nhjdataAlBERT = pd.read_csv(file_path+'nhjdataAlBERT.csv')\n",
        "nytdataAlBERT=pd.read_csv(file_path+'nytdataAlBERT.csv')\n",
        "\n",
        "#RoBERTA\n",
        "allRoBERTA = pd.read_csv(file_path+'allRoBERTA.csv')\n",
        "\n",
        "firstdataRoBERTA = pd.read_csv(file_path+'firstdataRoBERTA.csv')\n",
        "zhihudataRoBERTA=pd.read_csv(file_path+'zhihudataRoBERTA.csv')\n",
        "nhjdataRoBERTA=pd.read_csv(file_path+'nhjdataRoBERTA.csv')\n",
        "nytdataRoBERTA=pd.read_csv(file_path+'nytdataRoBERTA.csv')\n",
        "\n",
        "#by data\n",
        "allfirstdata=pd.read_csv(file_path+'allfirstdata.csv')\n",
        "allzhihudata=pd.read_csv(file_path+'allzhihudata.csv')\n",
        "allnhjdata=pd.read_csv(file_path+'allnhjdata.csv')\n",
        "allnytdata=pd.read_csv(file_path+'allnytdata.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X1UJO_QCl5o"
      },
      "outputs": [],
      "source": [
        "allRoBERTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rFmR486w1Pz"
      },
      "outputs": [],
      "source": [
        "\n",
        "new_row_names = ['First_Data', 'Zhihu_Data', 'NHJ_Data','NYT_Data']  # Replace ... with your row names\n",
        "#new_row_names = ['BERT', 'AlBERT', 'RoBERTA']\n",
        "# Set new row names using set_index()\n",
        "column_name_to_remove = 'Unnamed: 0'\n",
        "merged_df.drop(columns=column_name_to_remove, inplace=True)\n",
        "merged_df= merged_df.set_index(pd.Index(new_row_names))\n",
        "\n",
        "csv_file_path = '/content/drive/My Drive/InfoRet/results/allRoBERTA.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/allBERT.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/allAlBERT.csv'\n",
        "\n",
        "# csv_file_path = '/content/drive/My Drive/InfoRet/results/allfirstdata.csv'\n",
        "# csv_file_path = '/content/drive/My Drive/InfoRet/results/allzhihudata.csv'\n",
        "# csv_file_path = '/content/drive/My Drive/InfoRet/results/allnhjdata.csv'\n",
        "#csv_file_path = '/content/drive/My Drive/InfoRet/results/allnytdata.csv'\n",
        "\n",
        "\n",
        "# Save DataFrame to CSV file in Google Drive\n",
        "merged_df.to_csv(csv_file_path, index=True)\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-5SK2zgP9Gi"
      },
      "source": [
        "## **plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3Q6rddIb8fb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# df = allfirstdata\n",
        "# df = allzhihudata\n",
        "# df = allnhjdata\n",
        "# df = allnytdata\n",
        "# df = allBERT\n",
        "# df = allAlBERT\n",
        "df = allRoBERTA\n",
        "df_dimensions = df[['All']]\n",
        "# Set the dimensions (column names) for plotting\n",
        "dimensions = df_dimensions.columns\n",
        "\n",
        "# Create a bar chart for each dimension\n",
        "for dim in dimensions:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    #plt.bar(['BERT', 'ALBERT', 'RoBERTa'], df_dimensions[dim], color=['blue', 'orange', 'green'])\n",
        "    plt.bar(['First_Data', 'Zhihu_Data', 'NHJ_Data','NYT_Data'], df_dimensions[dim], color=['blue', 'orange', 'green','violet'])\n",
        "    plt.title(f'Total Model Representation for RoBERTa MODEL')\n",
        "    plt.xlabel('DATA')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.ylim(0, 1)  # Adjust the y-axis limits if needed\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ0vJAUEe0H4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read CSV data into a DataFrame\n",
        "#df=allfirstdata\n",
        "#df=allzhihudata\n",
        "#df = allnhjdata\n",
        "#df = allnytdata\n",
        "#df = allBERT\n",
        "#df=allAlBERT\n",
        "df = allRoBERTA\n",
        "\n",
        "#df = pd.read_csv(pd.compat.StringIO(csv_data))\n",
        "#df = pd.read_csv('/content/drive/My Drive/InfoRet/results/allfirstdata.csv')\n",
        "df = df.rename(columns={'Unnamed: 0': 'model_name'})\n",
        "# Exclude the 'All' column and select only the relevant dimensions\n",
        "df_dimensions = df.drop(columns=['All'])\n",
        "selected_dimensions = df_dimensions.columns[1:21]  # Exclude 'Model Name'\n",
        "\n",
        "#Create a line plot for each model\n",
        "for model in ['First_Data', 'Zhihu_Data', 'NHJ_Data','NYT_Data']:\n",
        "  plt.plot(selected_dimensions, df_dimensions.loc[df['model_name'] == model].values[0][1:21], marker='o', label=model)\n",
        "\n",
        "# Customize the plot\n",
        "\n",
        "plt.title('Model Representation for Each Dimension of AlBERT model')\n",
        "plt.xlabel('Dimension')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels and align them to the right\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIhIIt2pLTp7"
      },
      "source": [
        "##**Making marix plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7UI1jIsLdTF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "file_path = '/content/drive/MyDrive/InfoRet/results/matrix/'\n",
        "\n",
        "#df=pd.read_csv(file_path+'confusion_matrices_firstdataRoBERTatotal.csv')\n",
        "#df=pd.read_csv(file_path+'confusion_matrices_zhihudataRoBERTatotal.csv')\n",
        "#df=pd.read_csv(file_path+'confusion_matrices_nhjdataRoBERTatotal.csv')\n",
        "#df=pd.read_csv(file_path+'confusion_matrices_nytdataRoBERTatotal.csv')\n",
        "\n",
        "#df=pd.read_csv(file_path+'confusion_matrices_firstdataBBUtotal.csv')\n",
        "#df=pd.read_csv(file_path+'confusion_matrices_zhihudataBBUtotal.csv')\n",
        "#df=pd.read_csv(file_path+'confusion_matrices_nhjdataBBUtotal.csv')\n",
        "#df=pd.read_csv(file_path+'confusion_matrices_nytdataBBUtotal.csv')\n",
        "\n",
        "#df=pd.read_csv(file_path+'confusion_matrices_firstdataAlBERTtotal.csv')\n",
        "#df=pd.read_csv(file_path+'confusion_matrices_zhihudataAlBERTtotal.csv')\n",
        "#df=pd.read_csv(file_path+'confusion_matrices_nhjdataAlBERTtotal.csv')\n",
        "df=pd.read_csv(file_path+'confusion_matrices_nytdataAlBERTtotal.csv')\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd3OacWhN4Q-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# Confusion matrix values\n",
        "confusion_matrix = np.array([[1333, 156], [63, 48]])\n",
        "\n",
        "# Display labels (optional, set to None if not needed)\n",
        "display_labels = [\"False\", \"True\"]\n",
        "\n",
        "# Create the display\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=display_labels)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "disp.plot(cmap='Blues', values_format='d')\n",
        "plt.title(\"Confusion Matrix NYT Data AlBERT\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdyA3AXZ69ZI"
      },
      "source": [
        "# **Make predictions with the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5zBTMzG6-4D"
      },
      "outputs": [],
      "source": [
        "\n",
        "#model_type=\"bert\"\n",
        "#model_name=\"google-bert/bert-base-uncased\" #This may be a Hugging Face Transformers compatible pre-trained model, a community model, or the path to a directory containing model files. (ref to https://simpletransformers.ai/docs/classification-models/ )\n",
        "model_path=f\"/content/drive/MyDrive/InfoRet/models/{model_name}/best_model/\"\n",
        "\n",
        "model_type=\"roberta\" #mlm _restrucer of model\n",
        "model_name=\"FacebookAI/roberta-base\"\n",
        "\n",
        "#model_type=\"albert\" #mlm+sop _restrucer of model\n",
        "#model_name=\"albert/albert-base-v2\"\n",
        "\n",
        "model = MultiLabelClassificationModel(model_type, model_path)\n",
        "# predictions, _ = model.predict(test_df['sentence'].tolist())\n",
        "predictions, raw_outputs = model.predict([\"factory farming allows for the production of cheap food, which is a necessity for families surviving on a low income. against We should ban factory farming\"])\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WSfvg9X7rhK"
      },
      "outputs": [],
      "source": [
        "values = [\n",
        "    \"Self-direction: thought\",\n",
        "    \"Self-direction: action\",\n",
        "    \"Stimulation\",\n",
        "    \"Hedonism\",\n",
        "    \"Achievement\",\n",
        "    \"Power: dominance\",\n",
        "    \"Power: resources\",\n",
        "    \"Face\",\n",
        "    \"Security: personal\",\n",
        "    \"Security: societal\",\n",
        "    \"Tradition\",\n",
        "    \"Conformity: rules\",\n",
        "    \"Conformity: interpersonal\",\n",
        "    \"Humility\",\n",
        "    \"Benevolence: caring\",\n",
        "    \"Benevolence: dependability\",\n",
        "    \"Universalism: concern\",\n",
        "    \"Universalism: nature\",\n",
        "    \"Universalism: tolerance\",\n",
        "    \"Universalism: objectivity\"\n",
        "]\n",
        "def get_labels(values, output):\n",
        "    # Initialize an empty list to store labels\n",
        "    labels = []\n",
        "\n",
        "    # Iterate over the output list and the corresponding values\n",
        "    for idx, val in enumerate(output[0]):\n",
        "        # If the value is 1, append the corresponding label to the list\n",
        "        if val == 1:\n",
        "            labels.append(values[idx])\n",
        "\n",
        "    return labels\n",
        "\n",
        "\n",
        "\n",
        "# Get the labels where the output value is 1\n",
        "labels = get_labels(values, predictions)\n",
        "print(labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uB378Vdfng5"
      },
      "source": [
        "# Further Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8T1fBwPn2o4"
      },
      "outputs": [],
      "source": [
        "pip install accelerate>=0.21.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5GRHE5UzaiU"
      },
      "outputs": [],
      "source": [
        "pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q39GtLZYfr8c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from simpletransformers.classification import MultiLabelClassificationModel\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "models_path = '/content/drive/MyDrive/InfoRet/further_training/'\n",
        "model_type = \"roberta\"\n",
        "model_name = \"FacebookAI/roberta-base\"\n",
        "\n",
        "# Load your training and validation DataFrames\n",
        "# Assuming you have train_df and val_df available\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize the texts\n",
        "train_texts = train_df['text'].tolist()\n",
        "tokenized_texts = tokenizer(train_texts, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "# Save the tokenized texts to a file\n",
        "train_data_path = '/content/drive/MyDrive/InfoRet/further_training/train_data.txt'\n",
        "with open(train_data_path, 'w') as f:\n",
        "    for input_ids in tokenized_texts['input_ids']:\n",
        "        f.write(tokenizer.decode(input_ids, skip_special_tokens=True) + '\\n')\n",
        "\n",
        "# Define dataset and data collator for MLM pretraining (not used for classification)\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=train_data_path,\n",
        "    block_size=128, # Adjust according to your data and GPU memory\n",
        ")\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")\n",
        "\n",
        "# Define model parameters\n",
        "num_labels = 20  # Number of labels for your classification task\n",
        "training_args = {\n",
        "    \"output_dir\": f\"{models_path}{model_name}\",\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"num_train_epochs\": 12,\n",
        "    \"train_batch_size\": 16,\n",
        "    \"evaluate_during_training\": True,\n",
        "    \"save_steps\": -1,  # Save checkpoint after each epoch\n",
        "    \"save_model_every_epoch\": False,\n",
        "    \"save_eval_checkpoints\": False,\n",
        "}\n",
        "\n",
        "# Initialize MultiLabelClassificationModel for fine-tuning\n",
        "model = MultiLabelClassificationModel(\n",
        "    model_type,\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    args=training_args,\n",
        "    use_cuda=True,  # Change to False if you're not using CUDA\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.train_model(train_df, eval_df=val_df)\n",
        "\n",
        "# Save the trained model\n",
        "model.save_model(f\"{models_path}{model_name}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MNQvJxTjWIR8"
      },
      "outputs": [],
      "source": [
        "# test result for each lable\n",
        "values = [\n",
        "    \"Self-direction: thought\",\n",
        "    \"Self-direction: action\",\n",
        "    \"Stimulation\",\n",
        "    \"Hedonism\",\n",
        "    \"Achievement\",\n",
        "    \"Power: dominance\",\n",
        "    \"Power: resources\",\n",
        "    \"Face\",\n",
        "    \"Security: personal\",\n",
        "    \"Security: societal\",\n",
        "    \"Tradition\",\n",
        "    \"Conformity: rules\",\n",
        "    \"Conformity: interpersonal\",\n",
        "    \"Humility\",\n",
        "    \"Benevolence: caring\",\n",
        "    \"Benevolence: dependability\",\n",
        "    \"Universalism: concern\",\n",
        "    \"Universalism: nature\",\n",
        "    \"Universalism: tolerance\",\n",
        "    \"Universalism: objectivity\"\n",
        "]\n",
        "\n",
        "models_path = '/content/drive/MyDrive/InfoRet/further_training/'\n",
        "model_type = \"roberta\"\n",
        "model_name = \"FacebookAI/roberta-base\"\n",
        "\n",
        "def write_pr_rc_f1_to_file(path, pr, rc, f1):\n",
        "    with open(f'{path}/metrics.txt', \"w\") as f:\n",
        "        f.write(f\"pr: {str(pr)}\")\n",
        "        f.write(\"\\n\")\n",
        "        f.write(f\"rc:  {str(rc)}\")\n",
        "        f.write(\"\\n\")\n",
        "        f.write(f\"f1: {str(f1)}\")\n",
        "\n",
        "\n",
        "\n",
        "def write_text(filename, text):\n",
        "    with open(filename, \"w\") as f:\n",
        "        # f.write(text)\n",
        "        f.write(json.dumps(text))\n",
        "        f.write('\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "result_path =f'/content/drive/MyDrive/InfoRet/further_training/{model_name}/best_model/'\n",
        "\n",
        "predictions, _ = model.predict(test_df['text'].tolist())\n",
        "gold_label = test_df['labels'].tolist()\n",
        "pr, rc, f1, sup = precision_recall_fscore_support(gold_label, predictions, average=\"macro\",  zero_division=1)\n",
        "write_pr_rc_f1_to_file(path=f\"{result_path}\", pr=pr, rc=rc, f1=f1)\n",
        "\n",
        "report = classification_report(gold_label, predictions, target_names=values, output_dict=True,  zero_division=1)\n",
        "print(report)\n",
        "write_text(f\"{result_path}/classification_report_metric.txt\", report)\n",
        "\n",
        "print(f\"clssification_on_{model_name} is done!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkBceRXtfaQEqiokqlEXjD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}